{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elenab/Library/r-miniconda-arm64/envs/basilica-env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pybasilica.run as run\n",
    "import torch\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sklearn.metrics\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from pyro.distributions import constraints\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "m_g = pd.read_csv(\"test_datasets/counts_sbs.N150.G3.csv\")\n",
    "m_sbs = m_g.drop([\"groups\"], axis=1)\n",
    "g_sbs = m_g[\"groups\"].tolist() \n",
    "cosmic_sbs = pd.read_csv(\"test_datasets/COSMIC_filt.csv\", index_col=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_g = pd.read_csv(\"test_datasets/counts_dbs.N150.G3.csv\")\n",
    "m_dbs = m_g.drop([\"groups\"], axis=1)\n",
    "g_dbs = m_g[\"groups\"].tolist()\n",
    "cosmic_dbs = pd.read_csv(\"test_datasets/COSMIC_dbs.csv\", index_col=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ELBO 49954.599999: 100%|██████████| 2000/2000 [00:05<00:00, 335.06it/s]\n",
      "ELBO 46872.140686: 100%|██████████| 2000/2000 [00:05<00:00, 342.47it/s]\n"
     ]
    }
   ],
   "source": [
    "obj_sbs = run.fit(\n",
    "    x=m_sbs, \n",
    "    k_list=[3,4], \n",
    "    lr=0.005, \n",
    "    optim_gamma=0.1,\n",
    "    n_steps=2000, \n",
    "    # cluster=[3],\n",
    "    dirichlet_prior=True,\n",
    "    beta_fixed=cosmic_sbs.loc[[\"SBS1\",\"SBS5\"]], \n",
    "    store_parameters = True, \n",
    "    seed_list=[30],\n",
    "    nonparametric=False,\n",
    "    store_fits=True, enumer=\"parallel\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ELBO 132427.058856: 100%|██████████| 1000/1000 [00:02<00:00, 361.19it/s]\n"
     ]
    }
   ],
   "source": [
    "obj_dbs = run.fit(\n",
    "    x=m_dbs, \n",
    "    k_list=3, \n",
    "    lr=0.005, \n",
    "    optim_gamma=0.1,\n",
    "    n_steps=1000, \n",
    "    # cluster=[3],\n",
    "    dirichlet_prior=True,\n",
    "    beta_fixed=cosmic_dbs.loc[[\"DBS3\",\"DBS5\"]], \n",
    "    store_parameters = True, \n",
    "    seed_list=[30],\n",
    "    nonparametric=False,\n",
    "    store_fits=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ELBO -404.927028: 100%|██████████| 3000/3000 [00:07<00:00, 409.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9.9903e-01, 8.7136e-04, 1.0178e-04], dtype=torch.float64)\n",
      "tensor([9.9903e-01, 8.7136e-04, 1.0178e-04], dtype=torch.float64)\n",
      "tensor([9.9903e-01, 8.7136e-04, 1.0178e-04], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "input_alpha = [obj_sbs.params[\"alpha\"]]\n",
    "# input_alpha = [obj_sbs.params[\"alpha\"], obj_dbs.params[\"alpha\"]]\n",
    "obj_clust = run.fit(\n",
    "    alpha=input_alpha,\n",
    "    lr=0.005, \n",
    "    # optim_gamma=0.1,\n",
    "    n_steps=3000, \n",
    "    cluster=[3],\n",
    "    store_parameters=False, \n",
    "    hyperparameters={\"scale_factor_alpha\":1,\n",
    "                     \"scale_factor_centroid\":1000},\n",
    "    seed_list=[30],\n",
    "    nonparametric=False,\n",
    "    store_fits=True, \n",
    "    # enumer=\"sequential\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted_grps = obj_clust.groups \n",
    "init_grps = obj_clust.init_params[\"init_clusters\"] \n",
    "sklearn.metrics.normalized_mutual_info_score(fitted_grps, init_grps) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.39256328 0.57196712 0.0354696 ]\n",
      "[9.99026864e-01 8.71355663e-04 1.01779842e-04]\n"
     ]
    }
   ],
   "source": [
    "# print(np.mean(obj_clust.init_params[\"pi\"], axis=0))\n",
    "print(obj_clust.init_params[\"pi\"])\n",
    "print(obj_clust.params[\"pi\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0_SBS1    0_SBS5          0_D1          0_D2      0_D3      0_D4\n",
      "0  0.156846  0.000066  7.465467e-01  6.492421e-03  0.071960  0.018089\n",
      "1  0.204032  0.000002  3.020744e-06  1.474144e-06  0.000003  0.795959\n",
      "2  0.474104  0.013175  3.455439e-21  1.248418e-08  0.512140  0.000581\n",
      "     0_SBS1        0_SBS5          0_D1          0_D2          0_D3  \\\n",
      "0  0.182267  4.731494e-02  4.903169e-01  7.956645e-02  1.039038e-01   \n",
      "1  0.204032  7.964956e-11  1.363335e-10  6.645202e-11  1.242184e-10   \n",
      "2  0.475026  1.180196e-02  2.026300e-25  7.320904e-13  5.131715e-01   \n",
      "\n",
      "           0_D4  \n",
      "0  9.663107e-02  \n",
      "1  7.959676e-01  \n",
      "2  8.410459e-08  \n"
     ]
    }
   ],
   "source": [
    "print(obj_clust.init_params[\"alpha_prior\"])\n",
    "print(obj_clust.params[\"alpha_prior\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0]\n",
      "[2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(obj_clust.groups) \n",
    "print(obj_clust.init_params[\"init_clusters\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.639934e-15</td>\n",
       "      <td>1.157849e-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.204006e-15</td>\n",
       "      <td>1.090216e-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.481581e-14</td>\n",
       "      <td>4.194270e-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.827017e-14</td>\n",
       "      <td>9.403916e-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.448345e-15</td>\n",
       "      <td>1.371146e-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.510214e-16</td>\n",
       "      <td>8.211030e-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9.508093e-16</td>\n",
       "      <td>7.172341e-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>1.0</td>\n",
       "      <td>6.124291e-16</td>\n",
       "      <td>6.325948e-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.335639e-15</td>\n",
       "      <td>7.452509e-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.327091e-15</td>\n",
       "      <td>1.308844e-25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0             1             2\n",
       "0    1.0  3.639934e-15  1.157849e-23\n",
       "1    1.0  3.204006e-15  1.090216e-23\n",
       "2    1.0  2.481581e-14  4.194270e-23\n",
       "3    1.0  1.827017e-14  9.403916e-24\n",
       "4    1.0  2.448345e-15  1.371146e-23\n",
       "..   ...           ...           ...\n",
       "145  1.0  5.510214e-16  8.211030e-26\n",
       "146  1.0  9.508093e-16  7.172341e-26\n",
       "147  1.0  6.124291e-16  6.325948e-26\n",
       "148  1.0  1.335639e-15  7.452509e-26\n",
       "149  1.0  1.327091e-15  1.308844e-25\n",
       "\n",
       "[150 rows x 3 columns]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj_clust.params[\"post_probs\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from torch.distributions import constraints\n",
    "import numpy as np\n",
    "\n",
    "# model\n",
    "def model(data, K):\n",
    "    N = len(data)\n",
    "    hidden_dim = 2\n",
    "\n",
    "    # As in any clustering algorithm, the mixing proportions are the assignment probabilities of the cells.\n",
    "    # We sample the mixture weights from a Dirichlet distribution\n",
    "\n",
    "    weights = pyro.sample('mixture_weights', dist.Dirichlet(torch.ones(K)))\n",
    "\n",
    "    with pyro.plate('probabilities', K):  # cat_probs.size=(K,hidden_dim)\n",
    "        cat_probs = pyro.sample(\"cat_probabilities\", dist.Dirichlet(torch.ones(hidden_dim)))\n",
    "\n",
    "    cat_vector = torch.tensor(np.arange(hidden_dim) + 1, dtype=torch.float)\n",
    "    scale = pyro.sample(\"scale\", dist.Gamma(5, 1))\n",
    "\n",
    "    # likelihood\n",
    "    with pyro.plate(\"data\", N):\n",
    "        pyro.factor(\"lk\", log_lik(data, scale, weights, cat_vector, cat_probs, K))\n",
    "\n",
    "\n",
    "# loglikelihood\n",
    "def log_lik(data, scale, weights, cat_vector, cat_probs, K):\n",
    "    N = len(data)\n",
    "    hidden_dim = 2\n",
    "    data = data.reshape(N, 1, 1)\n",
    "    mean = scale * cat_vector.reshape(1, 1, hidden_dim)\n",
    "    weights = weights.reshape(1, K, 1)\n",
    "    cat_probs = cat_probs.reshape(1, K, hidden_dim)\n",
    "\n",
    "    lk = torch.log(weights) + dist.Poisson(mean).log_prob(data) + torch.log(cat_probs)\n",
    "    c = torch.max(torch.max(lk, dim=-1).values, dim=-1).values.reshape(N, 1, 1)\n",
    "    log_lik = c + torch.log(torch.exp(lk - c).sum(dim=-1).sum(dim=-1)).reshape(N, 1, 1)\n",
    "\n",
    "    return log_lik.sum()\n",
    "\n",
    "\n",
    "# guide\n",
    "def guide(data, K):\n",
    "    pi = pyro.param(\"q_mixture_weights\", create_params(data, K)[\"mixture_weights\"], constraint=constraints.simplex)\n",
    "    scale = pyro.param(\"q_scale\", create_params(data, K)[\"scale\"], constraint=constraints.positive)\n",
    "    probs = pyro.param(\"q_cat_probabilities\", create_params(data, K)[\"cat_probabilities\"],\n",
    "                       constraint=constraints.simplex)\n",
    "\n",
    "    print(probs)\n",
    "\n",
    "    with pyro.plate('probabilities', K):  # cat_probs.size=(K,hidden_dim)\n",
    "        cat_prob = pyro.sample(\"cat_probabilities\", dist.Delta(probs).to_event(1))\n",
    "    pyro.sample('mixture_weights', dist.Delta(pi).to_event(1))\n",
    "    pyro.sample(\"scale\", dist.Delta(scale))\n",
    "\n",
    "\n",
    "# initialization\n",
    "def create_params(data, K):\n",
    "    from sklearn.cluster import KMeans\n",
    "\n",
    "    N = len(data)\n",
    "    hidden_dim = 2\n",
    "\n",
    "    kmeans = KMeans(init=\"random\",\n",
    "                    n_clusters=K,\n",
    "                    n_init=10,\n",
    "                    max_iter=300,\n",
    "                    random_state=42)\n",
    "\n",
    "    kmeans.fit(data.reshape(N, 1))\n",
    "\n",
    "    categorical = torch.tensor(np.arange(hidden_dim) + 1, dtype=torch.float)\n",
    "    mean_categorical = categorical.mean()\n",
    "    mean_clusters = kmeans.cluster_centers_.mean()\n",
    "\n",
    "    # initialize scale\n",
    "    scale = mean_clusters / mean_categorical\n",
    "    mean = scale * categorical.reshape(1, hidden_dim)\n",
    "\n",
    "    # initialize mixing proportions and cat_probs\n",
    "    Prob = torch.ones(K, hidden_dim) / hidden_dim\n",
    "    mixing_proportions = torch.ones(K) / K\n",
    "\n",
    "    for k in range(K):\n",
    "        subset = []\n",
    "        for i in range(len(data)):\n",
    "            if kmeans.labels_[i] == k:\n",
    "                subset.append(data[i])\n",
    "\n",
    "        n = len(subset)\n",
    "        dataset = torch.tensor(subset).reshape(n, 1)\n",
    "        mixing_proportions[k] = len(subset) / len(data)\n",
    "\n",
    "        p = dist.Poisson(mean).log_prob(dataset)\n",
    "        c = torch.max(p, dim=0).values.reshape(1, hidden_dim)\n",
    "        p = c + torch.log(torch.exp(p - c).sum(dim=0)).reshape(1, hidden_dim)\n",
    "        c = torch.max(p, dim=-1).values\n",
    "        Norm = c + torch.log(torch.exp(p - c).sum(dim=-1))\n",
    "        Prob[k] = torch.exp(p - Norm)\n",
    "\n",
    "    params = {\"mixture_weights\": mixing_proportions, \"scale\": scale, \"cat_probabilities\": Prob}\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "# cluster assignments\n",
    "def cluster_assignments(data, K, pi, scale, categorical):\n",
    "    N = len(data)\n",
    "\n",
    "    mean = scale * categorical.reshape(1, K)\n",
    "    pi = pi.reshape(1, K)\n",
    "    data = data.reshape(N, 1)\n",
    "\n",
    "    Prob = torch.log(pi) + dist.Poisson(mean).log_prob(data)  # Prob.size=(N,K)\n",
    "    c = torch.max(Prob, dim=-1).values.reshape(N, 1)\n",
    "    Norm = c + torch.log(torch.exp(Prob - c).sum(dim=-1)).reshape(N, 1)  # Norm.size= (N,1)\n",
    "\n",
    "    return torch.exp(Prob - Norm)\n",
    "\n",
    "\n",
    "#inference\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "\n",
    "def inference(model,guide,K,data,lr=0.05,num_steps=500):\n",
    "\n",
    "    pyro.clear_param_store()  # always clear the store before the inference\n",
    "\n",
    "    # learning global parameters\n",
    "    adam_params = {\"lr\": lr}\n",
    "    optimizer = Adam(adam_params)\n",
    "    elbo = Trace_ELBO()\n",
    "\n",
    "    svi = SVI(model, guide, optimizer, loss=elbo)\n",
    "\n",
    "    # inference\n",
    "    # do gradient steps\n",
    "    for step in range(num_steps):\n",
    "        loss = svi.step(data, K)  # get the loss function after a gradient step\n",
    "        if step % 10 == 0:\n",
    "            print(\"loss =\", loss)  # check the progress\n",
    "\n",
    "    print(\"final loss =\", svi.evaluate_loss(data, K))\n",
    "\n",
    "    parameters = {}\n",
    "    for key in pyro.get_param_store().get_all_param_names():\n",
    "        parameters.update({key: torch.tensor(pyro.param(key))})\n",
    "\n",
    "    \n",
    "    cat_probs = parameters[\"q_cat_probabilities\"]\n",
    "    scale = parameters[\"q_scale\"]\n",
    "    pi = parameters[\"q_mixture_weights\"]\n",
    "    categorical = torch.argmax(cat_probs, dim=-1) + 1\n",
    "    assignment_probs = cluster_assignments(data, K, pi, scale, categorical)\n",
    "    assignments = torch.argmax(assignment_probs, dim=-1)\n",
    "    parameters.update({\"categorical_variable\":categorical, \n",
    "                       \"assignment_probs\":assignment_probs,\n",
    "                       \"assignments\":assignments})\n",
    "\n",
    "    # print(\"parameters:\", parameters)\n",
    "    # print(\"categorical variable:\", categorical)\n",
    "    # print(\"assignment_probs:\", assignment_probs)\n",
    "    # print(\"assignments\", assignments)\n",
    "\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plotting(data, K, assignments, bins=20):\n",
    "    fig, axes = plt.subplots(1, 1, figsize=(12, 10), sharey=True)\n",
    "    for k in range(K):\n",
    "        subset = []\n",
    "        for i in range(len(data)):\n",
    "            if assignments[i] == k:\n",
    "                subset.append(data[i])\n",
    "\n",
    "        dataset = torch.tensor(subset)\n",
    "        axes.hist(dataset, bins=bins, fill=True)\n",
    "\n",
    "\n",
    "#data generator\n",
    "def data_generator(N, scale, categorical, mixing_proportions):\n",
    "    data = []\n",
    "    means = scale * categorical\n",
    "\n",
    "    for n in range(N):\n",
    "        cluster = dist.Categorical(mixing_proportions).sample()\n",
    "        data.append(dist.Poisson(means[cluster]).sample())\n",
    "\n",
    "    data = torch.tensor(data)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example\n",
    "N = 1000\n",
    "scale = 5\n",
    "categorical = torch.tensor([1.,5.])\n",
    "K = len(categorical)\n",
    "mixing_proportions = torch.tensor([0.3,0.7])\n",
    "data = data_generator(N, scale, categorical, mixing_proportions)\n",
    "\n",
    "inferred_parameters = inference(model, guide, K, data, lr=0.05, num_steps=3)\n",
    "assignments = inferred_parameters[\"assignments\"]\n",
    "plotting(data, K, assignments, bins=20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inferred_parameters.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferred_parameters[\"q_cat_probabilities\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import dirichlet\n",
    "\n",
    "# Define the parameters\n",
    "alpha = [1, 1, 1]  # Adjust alpha values as needed\n",
    "power = 2.0  # Adjust the power parameter\n",
    "\n",
    "# Sample from the standard Dirichlet distribution\n",
    "sample = np.random.dirichlet(alpha)\n",
    "\n",
    "# Apply the transformation\n",
    "sample_away_from_mode = sample ** (1 / power)\n",
    "\n",
    "print(\"Sampled value away from the mode:\", sample_away_from_mode)\n",
    "print(\"Sampled:\", sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "\n",
    "# Define the parameters of the Dirichlet distribution\n",
    "alpha = torch.tensor([1.0, 5.0, 1.0])  # Replace with your alpha values\n",
    "\n",
    "# Sample from the Dirichlet distribution\n",
    "sampled_value = pyro.sample(\"sampled_value\", dist.Dirichlet(alpha))\n",
    "\n",
    "print(\"Sampled value away from the mode:\", sampled_value)\n",
    "\n",
    "# Find the mode of the Dirichlet distribution\n",
    "mode = torch.argmax(alpha)\n",
    "\n",
    "print(\"Mode\", mode)\n",
    "\n",
    "# Create a mask to zero out the mode value\n",
    "mask = torch.ones_like(sampled_value)\n",
    "mask[mode] = 0\n",
    "\n",
    "# Zero out the mode value\n",
    "sampled_value = sampled_value * mask\n",
    "\n",
    "# Renormalize to make it a valid probability distribution\n",
    "sampled_value = sampled_value / sampled_value.sum()\n",
    "\n",
    "print(\"Sampled value away from the mode:\", sampled_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = a_orig**(3)\n",
    "fig2, ax2 = plt.subplots()\n",
    "fig3, ax3 = plt.subplots()\n",
    "sns.histplot(a[:,0].tolist(), ax=ax2)\n",
    "sns.histplot(a[:,1].tolist(), ax=ax3)\n",
    "ax2.set_xlim(0,1)\n",
    "ax3.set_xlim(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(fixed, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed = torch.tensor(obj_sbs.beta_fixed.values)\n",
    "beta_w = torch.tensor(obj_sbs.params[\"beta_w\"].values)\n",
    "denovo = torch.tensor(obj_sbs.params[\"beta_d\"].values)\n",
    "cum_weights = torch.ones((obj_sbs.k_denovo, obj_sbs.k_fixed))/obj_sbs.k_fixed\n",
    "\n",
    "fixed_cum = obj_sbs._get_unique_beta_stick_breaking(beta_fixed=fixed, beta_denovo=None, beta_weights=cum_weights)\n",
    "fixed_cum = obj_sbs._norm_and_clamp(fixed_cum)\n",
    "\n",
    "print(torch.sum((fixed_cum * (torch.abs(fixed_cum - denovo)))) * torch.tensor(obj_sbs.x.values).sum())\n",
    "print(torch.sum((fixed_cum * (torch.abs(fixed_cum - denovo)))) * obj_sbs.x.shape[0] * obj_sbs.x.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_sbs.losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(obj_sbs.train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.distributions.Dirichlet(fixed_cum*1000).log_prob(denovo) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_sbs.gradient_norms.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## self.x.sum() * torch.sum(beta_fixed_cum * (1 - torch.abs(beta_fixed_cum - beta_denovo)))\n",
    "obj_sbs.params[\"beta_w\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## self.x.sum() * torch.sum(beta_fixed_cum * (torch.abs(beta_fixed_cum - beta_denovo)))\n",
    "obj_sbs.params[\"beta_w\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## self.x.sum() * torch.sum(beta_fixed_cum * (torch.abs(beta_fixed_cum - beta_denovo)))\n",
    "obj_sbs.params[\"beta_w\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## self.n_samples * self.contexts * pyro.distributions.Dirichlet(beta_fixed_cum*1000).to_event(1).log_prob(beta_denovo))\n",
    "obj_sbs.params[\"beta_w\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.param(\"beta_weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_sbs.params[\"beta_w\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_sbs.gradient_norms.keys() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_sbs.params[\"alpha\"].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_dn = 2\n",
    "k_f = 3\n",
    "n_samples = 5\n",
    "beta_weights = pyro.distributions.Dirichlet(torch.ones(k_dn, k_f+1)).sample()\n",
    "alpha_star = pyro.distributions.Dirichlet(torch.ones(n_samples, k_dn)).sample()\n",
    "print(\"beta weights\\n\", beta_weights)\n",
    "print(\"alpha star\\n\", alpha_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_weights[1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = torch.zeros((n_samples, k_dn+k_f))\n",
    "\n",
    "for n in range(n_samples):\n",
    "    for j in range(k_dn):\n",
    "        for r in range(k_f):\n",
    "            alpha[n, r] += torch.sum(alpha_star[n,j]) * beta_weights[j,r]\n",
    "        \n",
    "        for d in range(k_f, k_f+k_dn):\n",
    "            alpha[n, d] += torch.sum(alpha_star[n,j]) * beta_weights[j,-1]\n",
    "\n",
    "print(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_sbs.params[\"beta_w\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_sbs.params[\"beta_d\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_dbs = run.fit(\n",
    "    x=m_dbs, \n",
    "    k_list=3, \n",
    "    lr=0.005, \n",
    "    optim_gamma=0.1,\n",
    "    n_steps=10, \n",
    "    # cluster=6, \n",
    "    dirichlet_prior=True,\n",
    "    beta_fixed=cosmic_dbs.loc[[\"DBS4\"]], \n",
    "    hyperparameters={\"alpha_sigma\":.15, \"alpha_p_sigma\":1., \"alpha_p_conc0\":0.6, \n",
    "                     \"alpha_p_conc1\":0.6, \"alpha_rate\":1., \"pi_conc0\":0.5, \"alpha_conc\":100,\n",
    "                     \"scale_factor_alpha\":10000, \"scale_factor_centroid\":1000, \"scale_tau\":0},\n",
    "    enforce_sparsity = True, \n",
    "    reg_weight=0., \n",
    "    store_parameters = True, \n",
    "    seed_list=[92],\n",
    "    nonparametric=True,\n",
    "    store_fits=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_sbs = obj_sbs.params[\"alpha\"] \n",
    "alpha_dbs = obj_dbs.params[\"alpha\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = [alpha_sbs, alpha_dbs] \n",
    "input_tensor = [torch.tensor(alpha_sbs.values), torch.tensor(alpha_dbs.values)]\n",
    "max_shape = max([i.shape[1] for i in input_tensor])\n",
    "# stacked = torch.stack(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixture = run.fit(\n",
    "    alpha=input, \n",
    "    lr=0.005, \n",
    "    optim_gamma=0.1,\n",
    "    n_steps=3000,\n",
    "    cluster=5, \n",
    "    hyperparameters={\"alpha_sigma\":.15, \"alpha_p_sigma\":1., \"alpha_p_conc0\":0.6, \n",
    "                     \"alpha_p_conc1\":0.6, \"alpha_rate\":1., \"pi_conc0\":0.5, \"alpha_conc\":100,\n",
    "                     \"scale_factor_alpha\":10000, \"scale_factor_centroid\":1000, \"scale_tau\":0},\n",
    "    store_parameters = True, \n",
    "    seed_list=[92],\n",
    "    nonparametric=True,\n",
    "    store_fits=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def mix_weights(beta):\n",
    "    '''\n",
    "    Function used for the stick-breaking process.\n",
    "    '''\n",
    "    print(\"beta =\", beta)\n",
    "    beta1m_cumprod = (1 - beta).cumprod(-1)\n",
    "    print(\"beta1m_cumprod =\", beta1m_cumprod)\n",
    "    res1 = F.pad(beta, (0, 1), value=1)\n",
    "    res2 = F.pad(beta1m_cumprod, (1, 0), value=1)\n",
    "    res = res1 * res2\n",
    "    print(f\"res1 = {res1}, res2 = {res2}, res = {res}\\n\")\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = 6\n",
    "with pyro.plate(\"beta_plate\", cluster-1):\n",
    "    pi_beta = pyro.sample(f\"beta\", pyro.distributions.Beta(1, 1.1755e-36))\n",
    "    # pi_beta = torch.tensor([1.1755e-36, 2.1648e-18, 1.1755e-36, 6.6389e-33, 1.1755e-36])\n",
    "    print(\"pi_beta =\", pi_beta)\n",
    "    pi = mix_weights(pi_beta)\n",
    "\n",
    "print(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_star = torch.zeros(k_denovo, 96, dtype=torch.float64) \n",
    "for i in range(k_denovo):\n",
    "    tmp_sbs = torch.cat((ref_sbs, dn_sbs[i].unsqueeze(0)))\n",
    "    beta_star[i] = pi[i].unsqueeze(0).matmul(tmp_sbs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.distributions.Gamma(0.01, 0.01).sample((5,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.distributions.Dirichlet(torch.ones(5)).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(1 - pyro.distributions.Beta(1, 1e-10).sample((cluster-1,))).cumprod(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = torch.zeros((10,))\n",
    "pi[:5] = 5\n",
    "pi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_centr = mixture[0].params[\"alpha_prior\"]\n",
    "print(alpha_centr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sklearn.metrics.normalized_mutual_info_score(mixture.groups, g_sbs)) \n",
    "print(sklearn.metrics.normalized_mutual_info_score(mixture.groups, g_dbs)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(obj_sbs.params[\"scale_factor_centroid\"])\n",
    "print(obj_sbs.params[\"scale_factor_alpha\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_sbs.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_sbs.train_params[6][\"scale_factor_centroid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_sbs.params[\"pi_conc0\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=range(len(obj_sbs.likelihoods)), y=obj_sbs.likelihoods) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=range(len(obj_sbs.losses)), y=obj_sbs.losses) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: sns.scatterplot(x=range(len(obj_sbs.gradient_norms[\"scale_factor_centroid_param\"])), \n",
    "                     y=obj_sbs.gradient_norms[\"scale_factor_centroid_param\"]) \n",
    "except: print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: sns.scatterplot(x=range(len(obj_sbs.gradient_norms[\"scale_factor_alpha_param\"])), \n",
    "                     y=obj_sbs.gradient_norms[\"scale_factor_alpha_param\"]) \n",
    "except: print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: sns.scatterplot(x=range(len(obj_sbs.gradient_norms[\"alpha_prior_param\"])), y=obj_sbs.gradient_norms[\"alpha_prior_param\"]) \n",
    "except: print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: sns.scatterplot(x=range(len(obj_sbs.gradient_norms[\"alpha_prior_param\"])), y=obj_sbs.gradient_norms[\"alpha_prior_param\"]) \n",
    "except: print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: sns.scatterplot(x=range(len(obj_sbs.gradient_norms[\"pi_param\"])), y=obj_sbs.gradient_norms[\"pi_param\"]) \n",
    "except: print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: sns.scatterplot(x=range(len(obj_sbs.gradient_norms[\"pi_conc0_param\"])), y=obj_sbs.gradient_norms[\"pi_conc0_param\"]) \n",
    "except: print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: sns.scatterplot(x=range(len(obj_sbs.gradient_norms[\"alpha\"])), y=obj_sbs.gradient_norms[\"alpha\"]) \n",
    "except: print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: sns.scatterplot(x=range(len(obj_sbs.gradient_norms[\"beta_denovo\"])), y=obj_sbs.gradient_norms[\"beta_denovo\"])\n",
    "except: print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(np.array(obj_sbs.init_params[\"alpha_prior_param\"]), columns=obj_sbs.params[\"alpha\"].columns).plot.bar(stacked=True, legend=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: pd.DataFrame(np.array(obj_sbs.params[\"alpha_prior\"]), columns=obj_sbs.params[\"alpha_prior\"].columns).plot.bar(stacked=True, legend=False) \n",
    "except Exception as e: print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    for gid in set(np.array(obj_sbs.groups)):\n",
    "        tmp = [i for i, v in enumerate(obj_sbs.groups) if v == gid]\n",
    "        # tmp = [i for i, v in enumerate(obj_sbs.groups) if (v == gid and i in idxs)]\n",
    "        if len(tmp) == 0: continue\n",
    "        pd.DataFrame(np.array(obj_sbs.params[\"alpha\"]), columns=obj_sbs.params[\"alpha\"].columns, \n",
    "                     index=obj_sbs.params[\"alpha\"].index).iloc[tmp].plot.bar(stacked=True)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    obj_sbs.alpha.plot.bar(stacked=True, legend=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    for sbs in pd.concat((obj_sbs.params[\"beta_f\"], obj_sbs.params[\"beta_d\"])).index:\n",
    "        pd.concat((obj_sbs.params[\"beta_f\"], obj_sbs.params[\"beta_d\"])).loc[[sbs]].transpose().plot.bar()\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
