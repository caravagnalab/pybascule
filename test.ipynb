{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elenab/Library/r-miniconda-arm64/envs/basilica-env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pybasilica.run as run\n",
    "import torch\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sklearn.metrics\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from pyro.distributions import constraints\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "m_g = pd.read_csv(\"test_datasets/counts_sbs.N150.G3.csv\")\n",
    "m_sbs = m_g.drop([\"groups\"], axis=1)\n",
    "g_sbs = m_g[\"groups\"].tolist() \n",
    "cosmic_sbs = pd.read_csv(\"test_datasets/COSMIC_filt.csv\", index_col=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_g = pd.read_csv(\"test_datasets/counts_dbs.N150.G3.csv\")\n",
    "m_dbs = m_g.drop([\"groups\"], axis=1)\n",
    "g_dbs = m_g[\"groups\"].tolist()\n",
    "cosmic_dbs = pd.read_csv(\"test_datasets/COSMIC_dbs.csv\", index_col=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ELBO 48445.147023: 100%|██████████| 2000/2000 [00:05<00:00, 379.04it/s]\n",
      "ELBO 59385.313367:  43%|████▎     | 857/2000 [00:02<00:02, 386.30it/s] "
     ]
    }
   ],
   "source": [
    "obj_sbs = run.fit(\n",
    "    x=m_sbs, \n",
    "    k_list=[3,4], \n",
    "    lr=0.005, \n",
    "    optim_gamma=0.1,\n",
    "    n_steps=2000, \n",
    "    # cluster=[3],\n",
    "    dirichlet_prior=True,\n",
    "    beta_fixed=cosmic_sbs.loc[[\"SBS1\",\"SBS3\",\"SBS5\"]], \n",
    "    store_parameters = True, \n",
    "    seed_list=[30],\n",
    "    nonparametric=False,\n",
    "    store_fits=True, enumer=\"parallel\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_dbs = run.fit(\n",
    "    x=m_dbs, \n",
    "    k_list=3, \n",
    "    lr=0.005, \n",
    "    optim_gamma=0.1,\n",
    "    n_steps=1000, \n",
    "    # cluster=[3],\n",
    "    dirichlet_prior=True,\n",
    "    beta_fixed=cosmic_dbs.loc[[\"DBS3\",\"DBS5\"]], \n",
    "    store_parameters=False, \n",
    "    seed_list=[30],\n",
    "    store_fits=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_alpha = [obj_sbs.params[\"alpha\"], obj_dbs.params[\"alpha\"]]\n",
    "obj_clust = run.fit(\n",
    "    alpha=input_alpha,\n",
    "    lr=0.005, \n",
    "    # optim_gamma=0.1,\n",
    "    n_steps=3000, \n",
    "    cluster=[6],\n",
    "    seed_list=[33],\n",
    "    nonparametric=True,\n",
    "    store_fits=True,\n",
    "    autoguide=False\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, V, K, G = 1, 2, 5, 2\n",
    "\n",
    "expos = torch.tensor([[[0.298176768, 0.009485776, 0.680553348, 0.007903176, 0.003880932],\n",
    "                      [0.32536737, 0.31834484, 0.01290994, 0.34337785, 1e-37]]])\n",
    "\n",
    "centr = torch.tensor([[[2.663226e-01, 1.868571e-01, 1.686374e-01, 4.661891e-09, 3.781830e-01],\n",
    "                      [9.940815e-02, 7.912397e-02, 7.310225e-01, 9.044534e-02, 3.689002e-27]],\n",
    "                      [[0.3137514, 0.3310308, 0.1916676, 0.1635502, 1e-37],\n",
    "                       [4.090709e-01, 5.909291e-01, 5.109680e-11, 7.626013e-09, 1e-37]]])\n",
    "\n",
    "sf = torch.tensor([[35.59352, 52.37254],\n",
    "                   [43.17238, 47.30110]])\n",
    "\n",
    "pi = torch.tensor([0.26591883, 0.07771980])\n",
    "\n",
    "assert expos.shape == (N, V, K)\n",
    "assert centr.shape == (V, G, K)\n",
    "assert sf.shape == (V, G)\n",
    "\n",
    "thr = 0.01\n",
    "\n",
    "llik = torch.zeros(G, N)\n",
    "for g in range(G):\n",
    "    lprob_alpha = torch.zeros((V, N))\n",
    "    for v in range(V):\n",
    "        sf_v = sf[v, g]\n",
    "        alpha_prior_g = centr[g, v]\n",
    "\n",
    "        for n in range(N):\n",
    "            # print(f\"n={n}, v={v}, g={g}\")\n",
    "            # print(\"expos =\", expos[n,v,:])\n",
    "            # print(\"sf_v =\", sf_v)\n",
    "            idxs_keep = (expos[n,v,:] > thr).nonzero().squeeze()\n",
    "            # print(\"idxs =\", idxs_keep)\n",
    "            alpha_n = expos[n,v,idxs_keep] / expos[n,v,idxs_keep].sum()\n",
    "            # print(\"alpha_n =\", alpha_n)\n",
    "            alpha_tmp = alpha_prior_g * sf_v\n",
    "            alpha_c = alpha_tmp[idxs_keep]\n",
    "            # print(\"alpha_c =\", alpha_c, \"\\n\")\n",
    "            lprob_alpha[v,n] = dist.Dirichlet(alpha_c).log_prob(alpha_n)\n",
    "\n",
    "            # print(f\"n={n}, v={v}, g={g}\")\n",
    "            # print(\"expos =\", expos[n,v,:])\n",
    "            # print(\"sf_v =\", sf_v)\n",
    "            # alpha_n = expos[n,v, :].clone()\n",
    "            # alpha_n[alpha_n < thr] = 1e-10\n",
    "            # alpha_n = alpha_n / alpha_n.sum()\n",
    "            # print(\"alpha_n =\", alpha_n)\n",
    "            # alpha_c = alpha_prior_g * sf_v\n",
    "            # print(\"alpha_c =\", alpha_c)\n",
    "            # lprob_alpha[v,n] = dist.Dirichlet(alpha_c).log_prob(alpha_n)\n",
    "            # print(\"lprob_alpha[v,n] =\", lprob_alpha[v,n], \"\\n\")\n",
    "\n",
    "    # sum over independent variants and add log(pi)\n",
    "    print(\"lprob_alpha =\", torch.sum(lprob_alpha, dim=0))\n",
    "    llik[g, :] = torch.sum(lprob_alpha, dim=0) + torch.log(pi[g])\n",
    "    print(\"llik =\", llik[g, :], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsumexp(weighted_lp) -> torch.Tensor:\n",
    "    m = torch.amax(weighted_lp, dim=0).unsqueeze(0)  # the maximum value for each observation among the K values\n",
    "    summed_lk = m[-1] + torch.log(torch.sum(torch.exp(weighted_lp - m), axis=0))\n",
    "    return summed_lk\n",
    "\n",
    "ll = logsumexp(llik).unsqueeze(0)\n",
    "probs = torch.exp(llik - ll)\n",
    "torch.argmax(probs, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llik "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(obj_clust.params[\"scale_factor_centroid\"])\n",
    "print(obj_clust.params[\"z_tau\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_clust.groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_grps = obj_clust.groups \n",
    "init_grps = obj_clust.init_params[\"init_clusters\"] \n",
    "sklearn.metrics.normalized_mutual_info_score(fitted_grps, init_grps) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.round(obj_clust.init_params[\"pi\"], decimals=3))\n",
    "print(np.round(obj_clust.params[\"pi\"], decimals=3)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.pointplot(y=obj_clust.gradient_norms[\"scale_factor_centroid_param\"],\n",
    "#               x=np.arange(3000)) \n",
    "sns.pointplot(y=obj_clust.gradient_norms[\"AutoDelta.beta_pi\"],\n",
    "              x=np.arange(3000)) \n",
    "# sns.pointplot(y=obj_clust.gradient_norms[\"AutoDelta.latent_class\"],\n",
    "#               x=np.arange(1000)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pointplot(x=np.arange(3000), y=obj_clust.losses) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(obj_clust.groups) \n",
    "print(obj_clust.init_params[\"init_clusters\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import fsolve, minimize_scalar\n",
    "\n",
    "def _compute_dirichlet_variance(alpha):\n",
    "    alpha_hat = np.sum(alpha)\n",
    "    num = alpha * (alpha_hat - alpha)\n",
    "    denomin = alpha_hat**2 * (alpha_hat + 1)\n",
    "    return num / denomin\n",
    "\n",
    "def variance_condition(C, target, alpha):\n",
    "    variances = _compute_dirichlet_variance(alpha * C)\n",
    "    # print(f\"C={C}, max diff={max(np.abs(variances - target))}\")\n",
    "    return max(np.abs(variances - target))\n",
    "\n",
    "# Initial guess for C\n",
    "initial_C = 1\n",
    "# Calculate the smallest C for the given alpha and Y\n",
    "alpha = obj_clust.params[\"alpha_prior\"].values[0, :7]\n",
    "target = obj_clust.init_params[\"variances\"].values[0, :7]\n",
    "\n",
    "# Solve for C\n",
    "C_solution = minimize_scalar(fun=variance_condition, method=\"Bounded\", bounds=(1,500), args=(target, alpha))\n",
    "C_solution.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import fsolve\n",
    "\n",
    "def dirichlet_variance(alpha, alpha_hat):\n",
    "    # alpha_hat = np.sum(alpha)\n",
    "    num = alpha * (alpha_hat - alpha)\n",
    "    denomin = alpha_hat**2 * (alpha_hat + 1)\n",
    "    return num / denomin\n",
    "\n",
    "def optim_fn(c, true_var, alpha, alpha_hat):\n",
    "    variances = dirichlet_variance(alpha=alpha*c, alpha_hat=alpha_hat*c)\n",
    "    return np.abs(true_var - variances) + 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = dist.Dirichlet(torch.ones(5)).sample().numpy()\n",
    "true_var = np.ones(5) * 1e-4 \n",
    "print(f\"alpha = {alpha}, true_var = {true_var}\")\n",
    "sols = fsolve(func=optim_fn, x0=1.0, args=(true_var[0], alpha[0], np.sum(alpha))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solver(target, alpha_hat, alpha_k):\n",
    "    a = target*alpha_hat**3\n",
    "    b = target*alpha_hat**2\n",
    "    c = alpha_k**2 - alpha_k*alpha_hat\n",
    "\n",
    "    d = np.sqrt(b**2 - 4*a*c)\n",
    "    xs = np.array([(-b + d) / (2*a), (-b - d) / (2*a)])\n",
    "    return np.amax(xs)\n",
    "\n",
    "c = solver(target=true_var[0], alpha_hat=1, alpha_k=alpha[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirichlet_variance(alpha=alpha, alpha_hat=alpha.sum()) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
