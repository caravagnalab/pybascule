{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pybasilica import run\n",
    "from pybasilica import svi\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import pyro\n",
    "from sklearn.metrics import rand_score, mutual_info_score, normalized_mutual_info_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"./pybasilica/data/synthetic_data/simul.N100.G5.s23\"\n",
    "data = pd.read_csv(file+\"_counts.csv\", sep=',')\n",
    "groups_true = [i-1 for i in data[\"groups\"].values]\n",
    "data = data.drop(columns=[\"groups\"])\n",
    "N = data.shape[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_fixed_true = [\"SBS10b\", \"SBS1\",\"SBS5\",\"SBS22\",\"SBS12\",\"SBS3\"]\n",
    "beta_denovo_true = [\"SBS2\", \"SBS4\", \"SBS25\", \"SBS44\", \"SBS9\", \"SBS13\", \"SBS88\"]\n",
    "k_fixed_true = len(beta_fixed_true)\n",
    "k_denovo_true = len(beta_denovo_true)\n",
    "\n",
    "alpha_true = pd.read_csv(file+\"_alpha.csv\", sep=\",\")\n",
    "alpha_true = alpha_true[beta_fixed_true+beta_denovo_true]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogue = pd.read_csv(\"../simbasilica/script_test/COSMIC_v3.3.1_SBS_GRCh38.txt\", sep=\"\\t\", index_col=0).transpose()\n",
    "\n",
    "catalogue_fixed = catalogue.loc[beta_fixed_true]\n",
    "catalogue_denovo = catalogue.loc[beta_denovo_true]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine(obj, filt=True):\n",
    "    beta_est = pd.concat((obj.beta_denovo, obj.beta_fixed))\n",
    "    beta_est = beta_est.rename(index = lambda x: x + \"_E\")\n",
    "\n",
    "    out_cosine = cosine_similarity(np.concatenate((beta_est, catalogue_denovo, catalogue_fixed)))\n",
    "    names = list(beta_est.index) + beta_denovo_true + beta_fixed_true\n",
    "    out_cosine = pd.DataFrame(out_cosine, index=names, columns=names) \n",
    "    \n",
    "    if filt:\n",
    "        out_cosine = out_cosine.loc[list(beta_est.index)][beta_fixed_true+beta_denovo_true]\n",
    "    \n",
    "    return out_cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cosine(obj, filt=True): \n",
    "    out_cosine = get_cosine(obj, filt=filt)\n",
    "    \n",
    "    sns.heatmap(out_cosine, vmin=0., vmax=1., cmap=\"crest\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run with known groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1a = run.fit(data, k_list=k_denovo_true+k_fixed_true, verbose=False) \n",
    "x1b = run.fit(data, k_list=k_denovo_true+k_fixed_true, groups=groups_true, verbose=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine = get_cosine(x1a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cosine.idxmax())\n",
    "print(cosine.transpose().idxmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cosine(x1a, filt=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cosine(x1b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run with known groups and fixed catalogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2a = run.fit(data, k_list=k_denovo_true, beta_fixed=catalogue_fixed, verbose=False) \n",
    "x2b = run.fit(data, k_list=k_denovo_true, groups=groups_true, beta_fixed=catalogue_fixed, verbose=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cosine(x2a) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cosine(x2b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl = torch.zeros(N)\n",
    "for i in range(N):\n",
    "    kl[i] = torch.nn.functional.kl_div(torch.tensor(alpha_true.values)[i], x2b.alpha[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run with latent groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x3 = run.fit(data, k_list=k_denovo_true, cluster=5, beta_fixed=catalogue_fixed, enumer=False, verbose=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cosine(x3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rand_score(groups_true, x3.z.detach().numpy()))\n",
    "print(mutual_info_score(groups_true, x3.z.detach().numpy()))\n",
    "print(normalized_mutual_info_score(groups_true, x3.z.detach().numpy())) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run with full enumeration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x4 = run.fit(data, k_list=k_denovo_true, cluster=5, enumer=True, verbose=False, beta_fixed=catalogue_fixed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cosine(x4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = x4.alpha \n",
    "alpha_prior = x4.alpha_prior\n",
    "beta_d = x4.beta_denovo\n",
    "beta_f = x4.beta_fixed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = torch.tensor(x4.x.values)\n",
    "cluster = x4.cluster\n",
    "n_samples = x4.n_samples\n",
    "pi = x4.pi\n",
    "alpha_prior = x4.alpha_prior / (torch.sum(x4.alpha_prior, 1).unsqueeze(-1))\n",
    "try:\n",
    "    beta = torch.cat((torch.tensor(x4.beta_fixed.values), torch.tensor(x4.beta_denovo.values)), axis=0) \n",
    "except:\n",
    "    beta = torch.tensor(x4.beta_denovo.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_prior.sum(axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _logsumexp(weighted_lp) -> torch.Tensor:\n",
    "    '''\n",
    "    Returns `m + log( sum( exp( weighted_lp - m ) ) )`\n",
    "    - `m` is the the maximum value of weighted_lp for each observation among the K values\n",
    "    - `torch.exp(weighted_lp - m)` to perform some sort of normalization\n",
    "    In this way the `exp` for the maximum value will be exp(0)=1, while for the \n",
    "    others will be lower than 1, thus the sum across the K components will sum up to 1.\n",
    "    '''\n",
    "    m = torch.amax(weighted_lp, dim=0)  # the maximum value for each observation among the K values\n",
    "    summed_lk = m + torch.log(torch.sum(torch.exp(weighted_lp - m), axis=0))\n",
    "    return summed_lk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.zeros(n_samples)\n",
    "\n",
    "for n in range(n_samples):\n",
    "    m_n = M[n,:].unsqueeze(0)\n",
    "    ll_nk = torch.zeros((cluster, M.shape[1]))\n",
    "\n",
    "    for k in range(cluster):\n",
    "        # compute weighted log probability\n",
    "        ll_nk[k,:] = torch.log(pi[k]) + pyro.distributions.Poisson(torch.matmul( torch.matmul( torch.diag(torch.sum(m_n, axis=1).float()), alpha_prior[k,:].clone().detach().unsqueeze(0) ), beta.float() )).log_prob(m_n) \n",
    "\n",
    "    ll_nk_sum = ll_nk.sum(axis=1)\n",
    "    ll = _logsumexp(ll_nk_sum)\n",
    "    probs = torch.exp(ll_nk_sum - ll)\n",
    "\n",
    "    best_cl = torch.argmax(probs)\n",
    "    z[n] = best_cl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rand_score(groups_true, z))\n",
    "print(normalized_mutual_info_score(groups_true, z)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
