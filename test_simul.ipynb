{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pybascule import run\n",
    "from pybascule import svi\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import pyro\n",
    "from sklearn.metrics import rand_score, mutual_info_score, normalized_mutual_info_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"./pybascule/data/synthetic_data/simul.N100.G5.s23\"\n",
    "data = pd.read_csv(file+\"_counts.csv\", sep=',')\n",
    "groups_true = [i-1 for i in data[\"groups\"].values]\n",
    "data = data.drop(columns=[\"groups\"])\n",
    "N = data.shape[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_fixed_true = [\"SBS10b\", \"SBS1\",\"SBS5\",\"SBS22\",\"SBS12\",\"SBS3\"]\n",
    "beta_denovo_true = [\"SBS2\", \"SBS4\", \"SBS25\", \"SBS44\", \"SBS9\", \"SBS13\", \"SBS88\"]\n",
    "k_fixed_true = len(beta_fixed_true)\n",
    "k_denovo_true = len(beta_denovo_true)\n",
    "\n",
    "alpha_true = pd.read_csv(file+\"_alpha.csv\", sep=\",\")\n",
    "alpha_true = alpha_true[beta_fixed_true+beta_denovo_true]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogue = pd.read_csv(\"~/Google Drive/My Drive/work/signatures/simbascule/script_test/COSMIC_v3.3.1_SBS_GRCh38.txt\", sep=\"\\t\", index_col=0).transpose()\n",
    "\n",
    "catalogue_fixed = catalogue.loc[beta_fixed_true]\n",
    "catalogue_denovo = catalogue.loc[beta_denovo_true]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine(obj, filt=True):\n",
    "    beta_est = pd.concat((obj.beta_denovo, obj.beta_fixed))\n",
    "    beta_est = beta_est.rename(index = lambda x: x + \"_E\")\n",
    "\n",
    "    out_cosine = cosine_similarity(np.concatenate((beta_est, catalogue_denovo, catalogue_fixed)))\n",
    "    names = list(beta_est.index) + beta_denovo_true + beta_fixed_true\n",
    "    out_cosine = pd.DataFrame(out_cosine, index=names, columns=names) \n",
    "    \n",
    "    if filt:\n",
    "        out_cosine = out_cosine.loc[list(beta_est.index)][beta_fixed_true+beta_denovo_true]\n",
    "    \n",
    "    return out_cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cosine(obj, filt=True): \n",
    "    out_cosine = get_cosine(obj, filt=filt)\n",
    "    \n",
    "    sns.heatmap(out_cosine, vmin=0., vmax=1., cmap=\"crest\") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run without fixed signatures \n",
    "Compare the results with and without known groups as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1a = run.fit(data, k_list=k_denovo_true+k_fixed_true, verbose=False) \n",
    "x1b = run.fit(data, k_list=k_denovo_true+k_fixed_true, groups=groups_true, verbose=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_a = get_cosine(x1a)\n",
    "cosine_b = get_cosine(x1b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(set(cosine_a.transpose().idxmax().values)))\n",
    "print(len(set(cosine_b.transpose().idxmax().values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cosine(x1a, filt=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cosine(x1b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run with and fixed catalogue\n",
    "Compare the results with and without known groups as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2a = run.fit(data, k_list=k_denovo_true, beta_fixed=catalogue_fixed, verbose=False) \n",
    "x2b = run.fit(data, k_list=k_denovo_true, groups=groups_true, beta_fixed=catalogue_fixed, verbose=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cosine(x2a) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cosine(x2b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run with latent groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x3a = run.fit(data, k_list=k_denovo_true+k_fixed_true, cluster=5, enumer=False, verbose=False) \n",
    "x3b = run.fit(data, k_list=k_denovo_true, cluster=5, beta_fixed=catalogue_fixed, enumer=False, verbose=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cosine(x3a) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cosine(x3b) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rand_score(groups_true, x3a.z.detach().numpy()))\n",
    "print(mutual_info_score(groups_true, x3a.z.detach().numpy()))\n",
    "print(normalized_mutual_info_score(groups_true, x3a.z.detach().numpy())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rand_score(groups_true, x3b.z.detach().numpy()))\n",
    "print(mutual_info_score(groups_true, x3b.z.detach().numpy()))\n",
    "print(normalized_mutual_info_score(groups_true, x3b.z.detach().numpy())) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run with full enumeration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x4a = run.fit(data, k_list=k_denovo_true+k_fixed_true, cluster=5, enumer=\"sequential\", verbose=False) \n",
    "x4b = run.fit(data, k_list=k_denovo_true, cluster=5, enumer=\"sequential\", beta_fixed=catalogue_fixed, verbose=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.bincount(groups_true) / 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cosine(x4a) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cosine(x4b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_a = x4a.z \n",
    "z_b = x4b.z "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_a2 = compute_posterior_probs(x4a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x4a.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model with no beta_fixed - \", rand_score(groups_true, z_a), \". Model with beta_fixed - \", rand_score(groups_true, z_a)) \n",
    "print(\"Model with no beta_fixed - \", normalized_mutual_info_score(groups_true, z_a), \". Model with beta_fixed - \", normalized_mutual_info_score(groups_true, z_b)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot([float(pyro.distributions.HalfNormal(.2).sample()) for _ in range(1000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _logsumexp(weighted_lp) -> torch.Tensor:\n",
    "    '''\n",
    "    Returns `m + log( sum( exp( weighted_lp - m ) ) )`\n",
    "    - `m` is the the maximum value of weighted_lp for each observation among the K values\n",
    "    - `torch.exp(weighted_lp - m)` to perform some sort of normalization\n",
    "    In this way the `exp` for the maximum value will be exp(0)=1, while for the \n",
    "    others will be lower than 1, thus the sum across the K components will sum up to 1.\n",
    "    '''\n",
    "    m = torch.amax(weighted_lp, dim=0)  # the maximum value for each observation among the K values\n",
    "    summed_lk = m + torch.log(torch.sum(torch.exp(weighted_lp - m), axis=0))\n",
    "    return summed_lk \n",
    "\n",
    "def get_params(x):\n",
    "    params = dict()\n",
    "    params[\"alpha\"] = x.alpha\n",
    "    params[\"alpha_prior\"] = x.alpha_prior\n",
    "\n",
    "    params[\"beta_d\"] = x.beta_denovo\n",
    "    params[\"beta_f\"] = x.beta_fixed\n",
    "\n",
    "    params[\"pi\"] = x.pi\n",
    "\n",
    "    return params\n",
    "\n",
    "def compute_posterior_probs(x):\n",
    "    params = get_params(x)\n",
    "    M = torch.tensor(x.x.values)\n",
    "    cluster = x.cluster\n",
    "    n_samples = x.n_samples\n",
    "    \n",
    "    # not necesary since already normalised\n",
    "    # alpha_prior = x4.alpha_prior / (torch.sum(x4.alpha_prior, 1).unsqueeze(-1))\n",
    "    try:\n",
    "        beta = torch.cat((torch.tensor(params[\"beta_f\"].values), torch.tensor(params[\"beta_d\"].values)), axis=0) \n",
    "    except:\n",
    "        beta = torch.tensor(params[\"beta_d\"].values)\n",
    "    \n",
    "    z = torch.zeros(n_samples)\n",
    "\n",
    "    for n in range(n_samples):\n",
    "        m_n = M[n,:].unsqueeze(0)\n",
    "        ll_nk = torch.zeros((cluster, M.shape[1]))\n",
    "\n",
    "        for k in range(cluster):\n",
    "            muts_n = torch.sum(m_n, axis=1).float()  # muts for patient n\n",
    "            rate = torch.matmul( \\\n",
    "                torch.matmul( torch.diag(muts_n), params[\"alpha_prior\"][k,:].unsqueeze(0) ), \\\n",
    "                beta.float() )\n",
    "            \n",
    "            # compute weighted log probability\n",
    "            ll_nk[k,:] = torch.log(params[\"pi\"][k]) + pyro.distributions.Poisson( rate ).log_prob(m_n) \n",
    "\n",
    "        ll_nk_sum = ll_nk.sum(axis=1)  # sum over the contexts -> reaches a tensor of shape (n_clusters)\n",
    "        ll = _logsumexp(ll_nk_sum)\n",
    "        probs = torch.exp(ll_nk_sum - ll)\n",
    "\n",
    "        best_cl = torch.argmax(probs)\n",
    "        z[n] = best_cl \n",
    "\n",
    "    return z"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
