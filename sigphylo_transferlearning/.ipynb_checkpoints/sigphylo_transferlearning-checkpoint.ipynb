{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#toy model bayesian signature inference\n",
    "\n",
    "import torch\n",
    "import pyro\n",
    "import pyro.distributions as dist \n",
    "import numpy as np\n",
    "\n",
    "#define the model in terms of M = phylogenetic signature matrix, K_denovo = number of signatures we want to infer de novo,\n",
    "# K_fixed = number of signatures we already know. The fixed signatures are described by the fixed matrix Beta_fixed. \n",
    "# We provide an adjacency matrix A with binary entries which specify the correlation structure imposed by the phylogeny. \n",
    "# K = K_denovo + K_fixed\n",
    "# M has dims (num_branches,96)\n",
    "# theta is a vector encoding the number of mutations for each branch\n",
    "# beta prior and alpha prior provide densities for the normal priors for Beta_denovo and alpha\n",
    "# beta prior has dims (K_denovo,96), alpha prior has dims (num_branches,K_denovo)\n",
    "\n",
    "\n",
    "def model_single_run(M,beta_fixed,K_denovo,beta_prior,alpha_prior):\n",
    "    \n",
    "    num_samples = M.size()[0]\n",
    "    \n",
    "    K_fixed = beta_fixed.size()[0]\n",
    "    \n",
    "    theta = torch.sum(M,axis = 1)\n",
    "    \n",
    "    #parametrize the activity matrix as theta*alpha, where theta encodes the total number of mutations of the branches \n",
    "    # and alpha's are percentages of signature activity\n",
    "    \n",
    "   # sample alpha from a normal distribution using alpha prior\n",
    "\n",
    "    alpha = pyro.sample(\"activities\", dist.Normal(alpha_prior,1))\n",
    "    \n",
    "    # sample the extra signature profiles from normal distribution\n",
    "    \n",
    "    beta_denovo = pyro.sample(\"extra_signatures\", dist.Normal(beta_prior,1))  \n",
    "    \n",
    "    # enforce non negativity\n",
    "    \n",
    "    alpha = torch.exp(alpha)\n",
    "    \n",
    "    beta_denovo = torch.exp(beta_denovo)\n",
    "    \n",
    "    #normalize\n",
    "    \n",
    "    alpha = alpha/(torch.sum(alpha, 1).unsqueeze(-1))\n",
    "    \n",
    "    beta_denovo = beta_denovo/(torch.sum(beta_denovo,1).unsqueeze(-1))\n",
    "    \n",
    "    # build full beta matrix\n",
    "\n",
    "    beta = torch.cat((beta_fixed,beta_denovo), axis = 0)\n",
    "    \n",
    "    # write the likelihood\n",
    "\n",
    "    with pyro.plate(\"context\",96):\n",
    "        \n",
    "        with pyro.plate(\"sample\",num_samples):\n",
    "    \n",
    "            pyro.sample(\"obs\", dist.Poisson(torch.matmul(torch.matmul(torch.diag(theta),alpha),beta)), obs = M)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.infer.autoguide import AutoDelta\n",
    "from pyro.infer.autoguide.initialization import init_to_sample\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam  \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# perform MAP estimates\n",
    " \n",
    "def guide_single_run(M,beta_fixed,K_denovo,beta_prior,alpha_prior):\n",
    "    \n",
    "    alpha = pyro.param(\"alpha\", dist.Normal(alpha_prior,1).sample())\n",
    "    \n",
    "    beta = pyro.param(\"beta\", dist.Normal(beta_prior,1).sample())\n",
    "    \n",
    "    pyro.sample(\"activities\", dist.Delta(alpha))\n",
    "    \n",
    "    pyro.sample(\"extra_signatures\", dist.Delta(beta)) \n",
    "    \n",
    "    \n",
    "# SVI\n",
    "\n",
    "def inference_single_run(M,beta_fixed,K_denovo,beta_prior,alpha_prior,lr=0.05,num_steps = 200):\n",
    "\n",
    "    pyro.clear_param_store()  # always clear the store before the inference\n",
    "\n",
    "    # learning global parameters\n",
    "\n",
    "    adam_params = {\"lr\": lr}\n",
    "    optimizer = Adam(adam_params)\n",
    "    elbo = Trace_ELBO()\n",
    "\n",
    "    svi = SVI(model_single_run, guide_single_run, optimizer, loss=elbo)\n",
    "\n",
    "#   inference\n",
    "\n",
    "#   do gradient steps\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        \n",
    "        loss = svi.step(M,beta_fixed,K_denovo,beta_prior,alpha_prior) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_transfer_coeff(parameters,M,beta_fixed,A,hyper_lambda):\n",
    "    \n",
    "    \n",
    "    alpha = torch.exp(parameters[\"alpha\"])\n",
    "    alpha = alpha/(torch.sum(alpha,1).unsqueeze(-1))\n",
    "    \n",
    "    beta_denovo = torch.exp(parameters[\"beta\"])\n",
    "    beta_denovo = beta_denovo/(torch.sum(beta_denovo,1).unsqueeze(-1))\n",
    "    \n",
    "    beta = torch.cat((beta_fixed,beta_denovo),axis = 0)\n",
    "    \n",
    "    theta = torch.sum(M,axis = 1)\n",
    "    \n",
    "    num_samples = M.size()[0]\n",
    "    \n",
    "    cos = torch.zeros(num_samples,num_samples)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        \n",
    "        for j in range(num_samples):\n",
    "        \n",
    "            if A[i,j] == 1:\n",
    "                \n",
    "                M_r = theta[i]*torch.matmul(alpha[j],beta)\n",
    "                \n",
    "                cos[i,j] = torch.dot(M[i],M_r)/(torch.norm(M[i])*torch.norm(M_r))\n",
    "                \n",
    "    \n",
    "    \n",
    "    w = cos/(torch.sum(cos,1).unsqueeze(-1))\n",
    "    \n",
    "    transfer_coeff = torch.zeros(num_samples,num_samples)\n",
    "    \n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        \n",
    "        for j in range(num_samples):\n",
    "            \n",
    "            if i==j:\n",
    "                \n",
    "                transfer_coeff[i,j]  = (1-hyper_lambda)*w[i,j]\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                transfer_coeff[i,j]  = hyper_lambda*w[i,j]\n",
    "     \n",
    "    \n",
    "    return transfer_coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_inference(M,A,beta_fixed,K_denovo,hyper_lambda,lr = 0.05,steps_per_iteration = 200,num_iterations = 10):\n",
    "\n",
    "\n",
    "    # first indipendent run\n",
    "    \n",
    "    num_samples = M.size()[0]\n",
    "    \n",
    "    K_fixed = beta_fixed.size()[0]\n",
    "            \n",
    "    # step 0 : indipendent inference\n",
    "    \n",
    "    print(\"iteration \",0)\n",
    "            \n",
    "    alpha_prior = dist.Normal(torch.zeros(num_samples,K_denovo + K_fixed),1).sample()\n",
    "            \n",
    "    beta_prior = dist.Normal(torch.zeros(K_denovo,96),1).sample()\n",
    "\n",
    "    inference_single_run(M,beta_fixed,K_denovo,beta_prior,alpha_prior,lr = lr, \n",
    "                                            num_steps = steps_per_iteration)\n",
    "    \n",
    "    \n",
    "    # do iterations transferring alpha's\n",
    "            \n",
    "    for i in range(num_iterations):    \n",
    "                    \n",
    "        print(\"iteration \", i+1)\n",
    "            \n",
    "             \n",
    "            #extract inferred alpha's and beta'a from pyro store\n",
    "            \n",
    "        parameters={}\n",
    "        \n",
    "        \n",
    "        for key in pyro.get_param_store().get_all_param_names() :\n",
    "                \n",
    "            parameters.update({key : torch.tensor(pyro.param(key))})\n",
    "                \n",
    "        \n",
    "        alpha_prior = parameters[\"alpha\"]\n",
    "        \n",
    "        beta_prior = parameters[\"beta\"]\n",
    "            \n",
    "       \n",
    "       #calculate transfer coeff\n",
    "        \n",
    "        transfer_coeff = calculate_transfer_coeff(parameters,M,beta_fixed,A,hyper_lambda)\n",
    "            \n",
    "            \n",
    "            #update alpha prior with transfer coeff\n",
    "            \n",
    "        alpha_prior = torch.matmul(transfer_coeff,alpha_prior)\n",
    "            \n",
    "            \n",
    "            # do inference with updates alpha_prior and beta_prior\n",
    "    \n",
    "        inference_single_run(M,beta_fixed,K_denovo,beta_prior,alpha_prior,lr = lr, \n",
    "                                            num_steps = steps_per_iteration)\n",
    "    \n",
    "            \n",
    "    #save final inference\n",
    "\n",
    "    alpha_denovo = torch.exp(parameters[\"alpha\"])\n",
    "    alpha_denovo = alpha_denovo/(torch.sum(alpha_denovo,1).unsqueeze(-1))\n",
    "    \n",
    "    beta_denovo = torch.exp(parameters[\"beta\"])\n",
    "    beta_denovo = beta_denovo/(torch.sum(beta_denovo,1).unsqueeze(-1))\n",
    "    \n",
    "    SigPhylo = {\"alpha\" : alpha_denovo, \"beta\" : beta_denovo, \"lambda\" : hyper_lambda, \"K de novo\": K_denovo}\n",
    "                \n",
    "       \n",
    "    return SigPhylo            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "\n",
    "my_path = \"/Users/riccardobergamin/Documents/GitHub/SigPhylo/toy_model/\"\n",
    "import pandas as pd\n",
    "\n",
    "# load data\n",
    "X_1 = pd.read_csv(my_path + \"sample_1.csv\")\n",
    "X_2 = pd.read_csv(my_path + \"sample_2.csv\")\n",
    "c_1 = X_1.values[:,1:]\n",
    "c_2 = X_2.values[:,1:]\n",
    "M_1 = torch.tensor(np.array(c_1,dtype=float))\n",
    "M_2 = torch.tensor(np.array(c_2,dtype=float))\n",
    "M_1 = M_1.float()\n",
    "M_2 = M_2.float()\n",
    "# load clock-like signatures\n",
    "clock_like_signatures = pd.read_csv(my_path + \"beta_aging.csv\")\n",
    "beta_fixed = clock_like_signatures.values[:,1:]\n",
    "beta_fixed = torch.tensor(np.array(beta_fixed,dtype=float))\n",
    "beta_fixed = beta_fixed.float()\n",
    "# define adjacency matrix\n",
    "A = torch.ones(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  0\n",
      "iteration  1\n",
      "alpha tensor([-2.5893, -0.4575,  1.5708])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-21-fdad0f204f1c>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  parameters.update({key : torch.tensor(pyro.param(key))})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  2\n",
      "alpha tensor([-0.9305, -1.1592,  1.9921])\n",
      "iteration  3\n",
      "alpha tensor([-0.4993, -2.4958,  2.3085])\n",
      "iteration  4\n",
      "alpha tensor([-1.4134, -1.3559,  2.4939])\n",
      "iteration  5\n",
      "alpha tensor([-1.3471, -2.5649,  0.7761])\n",
      "iteration  6\n",
      "alpha tensor([-0.8285, -0.4652,  1.9562])\n",
      "iteration  7\n",
      "alpha tensor([-2.1639, -1.2003,  1.7462])\n",
      "iteration  8\n",
      "alpha tensor([-1.0960, -0.3752,  2.5205])\n",
      "iteration  9\n",
      "alpha tensor([-1.2584, -1.7445,  1.2285])\n",
      "iteration  10\n",
      "alpha tensor([-0.9742, -0.8802,  2.1420])\n",
      "iteration  11\n",
      "alpha tensor([-1.0866, -0.9796,  1.9942])\n",
      "iteration  12\n",
      "alpha tensor([-1.2319, -0.4938,  2.6316])\n",
      "iteration  13\n",
      "alpha tensor([-0.6621, -0.5165,  2.4695])\n",
      "iteration  14\n",
      "alpha tensor([-2.2499, -1.0505,  1.6626])\n",
      "iteration  15\n",
      "alpha tensor([-0.5546, -0.5876,  2.4214])\n",
      "iteration  16\n",
      "alpha tensor([-1.3920, -0.4873,  2.4530])\n",
      "iteration  17\n",
      "alpha tensor([-1.7748, -0.4154,  1.9103])\n",
      "iteration  18\n",
      "alpha tensor([-1.3085,  0.0162,  2.7495])\n",
      "iteration  19\n",
      "alpha tensor([-2.1197, -1.4670,  1.6483])\n",
      "iteration  20\n",
      "alpha tensor([-1.2743, -0.7605,  2.2977])\n",
      "iteration  21\n",
      "alpha tensor([-1.4217, -1.0771,  2.2157])\n",
      "iteration  22\n",
      "alpha tensor([-2.4507, -1.3936,  1.3321])\n",
      "iteration  23\n",
      "alpha tensor([-2.3939, -1.6264,  1.1660])\n",
      "iteration  24\n",
      "alpha tensor([-1.2240, -0.8407,  1.0445])\n",
      "iteration  25\n",
      "alpha tensor([-2.4721, -1.3223,  0.7894])\n",
      "iteration  26\n",
      "alpha tensor([-0.7387, -0.2991,  2.4638])\n",
      "iteration  27\n",
      "alpha tensor([-1.5173, -0.0323,  2.3976])\n",
      "iteration  28\n",
      "alpha tensor([-2.0672, -0.9255,  1.8198])\n",
      "iteration  29\n",
      "alpha tensor([-3.1111, -0.7931,  0.6851])\n",
      "iteration  30\n",
      "alpha tensor([-2.5718, -1.0870,  0.7102])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'alpha': tensor([[0.2343, 0.6640, 0.1017],\n",
       "         [0.0356, 0.1681, 0.7963],\n",
       "         [0.0312, 0.1378, 0.8310]]),\n",
       " 'beta': tensor([[1.0648e-01, 4.5214e-03, 3.2899e-04, 8.6477e-03, 1.7579e-03, 2.8735e-06,\n",
       "          8.0393e-04, 1.1044e-03, 2.0043e-05, 5.9519e-07, 4.6585e-07, 5.9171e-06,\n",
       "          4.9833e-03, 3.8154e-03, 3.6517e-03, 6.4200e-03, 7.2760e-07, 3.5336e-06,\n",
       "          1.3584e-06, 1.3621e-06, 4.1613e-05, 1.3985e-07, 1.7607e-06, 7.8309e-06,\n",
       "          3.4872e-01, 2.3027e-03, 2.5877e-03, 9.6920e-03, 1.5340e-04, 1.0096e-05,\n",
       "          1.1525e-05, 3.0896e-04, 2.1642e-06, 1.9043e-05, 9.2680e-07, 1.6290e-04,\n",
       "          2.6317e-03, 7.5643e-04, 1.5227e-03, 3.5926e-03, 5.7332e-05, 4.9895e-04,\n",
       "          5.4042e-06, 6.8096e-06, 2.1913e-05, 3.4003e-06, 9.5401e-05, 5.7111e-06,\n",
       "          2.0877e-01, 1.2689e-02, 1.0903e-02, 6.4838e-02, 3.1219e-03, 3.0251e-04,\n",
       "          2.1949e-05, 1.5593e-03, 1.5050e-03, 3.5424e-06, 2.6769e-07, 8.0531e-05,\n",
       "          7.4389e-03, 8.3791e-03, 8.9112e-03, 3.9443e-03, 1.4612e-03, 7.1009e-05,\n",
       "          1.3703e-03, 2.1628e-04, 6.5142e-04, 2.1589e-05, 2.4905e-04, 1.1328e-03,\n",
       "          1.0441e-01, 2.7384e-03, 1.9786e-03, 1.9636e-02, 1.4195e-03, 1.6155e-04,\n",
       "          5.1509e-04, 1.8965e-03, 2.2563e-03, 9.0387e-05, 5.1140e-07, 1.2051e-03,\n",
       "          1.5500e-03, 9.9543e-04, 4.0545e-03, 3.1975e-03, 8.7830e-07, 2.2562e-04,\n",
       "          6.8665e-06, 7.0107e-05, 7.7845e-05, 2.2613e-07, 3.5445e-05, 5.7698e-05]]),\n",
       " 'lambda': 0.5,\n",
       " 'K de novo': 1}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample 1\n",
    "\n",
    "full_inference(M_1,A,beta_fixed,K_denovo=1,hyper_lambda=0.5,lr = 0.005,steps_per_iteration = 500,num_iterations = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
